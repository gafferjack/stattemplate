---
title: "Tests of Significance, Version 2"
author: "Professor Sean Carver (Replace with Your Name)"
output: html_document
---

Remember the violin/box plot of the price of diamonds in the diamonds data set?


```{r load, message=F, echo=F, cache=FALSE}
require(ggplot2)
require(scales)
require(reshape2)
require(lubridate)
data(diamonds)
```

```{r meandia, echo = F}
ggplot(diamonds, aes(x=0, y=price)) + geom_violin() + geom_boxplot(width=0.1) + xlab("") + theme(axis.text.x=element_blank()) + stat_summary(fun.y=mean, geom="point", size=2, color="red") + scale_y_continuous(labels = dollar, breaks = seq(0, 20000, 1000))
```

The violin/box plot, shown above, illustrates the distribution of the price variable across the population represented in the data set.  Later, we will call this population of `r length(diamonds$price)` diamonds, the _reference or null population_ of diamonds.  But, while keeping the reference population in mind, we are going to work with _a sample_ of diamonds from a _possibly different population_ (for example, diamonds from another store---different from the one that housed the orignial population of `r length(diamonds$price)` diamonds).  We won't know the size of this new population, and for the moment, we will even leave the size of this sample unspecified.  Maybe the sample is only 4 diamonds, or maybe it's 400 diamonds (but actually, below, we will start with a sample of only 1 single diamond, before upping our sample size in a future lesson).  

**Our main question: "Are the diamonds from this new population, from which the sample was drawn, _more expensive, on average,_ than diamonds from our original, reference, or null population?"**  In other words, for example, does Zales Jewelers, sell more expensive diamonds, on average, than Helzberg Diamonds?  We would need data from these two stores to address this question.  

<span style="color:red"> Question: If we had data sets that involved prices of _all_ of the diamonds from Zales Jewlers, and prices of _all_ of the diamonds from Helzberg Diamonds, describe a procedure, with formulas, for determining precisely, beyond a shadow of a doubt, which Jewler sells more expensive diamonds, on average. </span>  This question should be easy, but our problem is harder.  We have all of the diamonds from one store, but only a sample from the other (requiring a one-sample test).  A related question asks what we would do if we only two samples, one from each store.

I am not going to tell you how I got the samples I use below---that information is totally irrelvant for the present purposes.  Suffice to say, I only have data from one store.  But I will say that it is possible to sample from other populations, given just the data we have.  Specifically, you can create new populations by simply restricting the reference population using other characteristics of diamonds described by the data set: color, cut, clarity, carat, etc.  

For example, we could ask, are diamonds with the D-color designation more expensive than diamonds overall.  If you know anything about diamonds, you might know that diamonds with this color designation are considered more valuable, all other things being equal, than diamonds of any other color designation.  So this question sounds like a no-brainer---except for the "all other things being equal" clause.  Without knowing anything about diamonds, it would be entirely conceivable to wonder if D-color diamonds might tend to be a lot smaller than other diamonds, so their price might tend to be less, on average, despite their superior color, simply due to their inferior weight (carat).  Of course, with the whole data set you could answer this question definitively---and without sampling.  And if you know anything about diamonds, you might know the answer already.  <span style="color:red"> Question: Describe a procedure in words, not code, for determining the answer to this question definitively---and without sampling, with the whole diamonds data set.</span>  

But the question I am asking here is different.  Here, you are given a _sample_ of diamonds, and their prices.  The sample comes from a larger population, and you know that it is drawn as a simple random sample from that population.  You don't know anything about this population other than the prices of the diamonds in that simple random sample.  The sampled-from population may be from another store, or it may be from a subset of diamonds from the reference population.  You don't know---and if you do know, these details don't matter for the problem at hand.  We address this question: are the diamonds in the new sampled-from population (the whole population, including the diamonds whose prices you don't know) more expensive, on average, than the diamonds in your reference population (the one that you do know about).  <span style="color:red"> Question: Why is impossible to answer this question with the information you have, definitively, and beyond a shadow of a doubt? </span>  

Note that while these questions may be criticized as being contrived, they remain very similar to important questions in other domains, such as: "Does a new drug work better, on average than an old drug," or: "Do students perform better with a new teaching method than with an old teaching method?"  For these questions, sampling is not always so contrived.  You can't know how all the people in the world with the disease you are studying (your population of interest) would respond to your drug---you can only know how the people you are studying in your sample would respond to the drug, the ones to whom you gave the drug.  For these problems, a **two sample** test might be more relevant.  A two sample test would involve, for example, both a sample of people using the new drug, and a sample of people using an old drug (or a placebo).  We will get to two sample tests soon enough, but we will start with a **one sample** test.  Here we compare one sample from an unknown distribution to a reference or null population that is fully known.

Our reference population consists of the entire data set of `r length(diamonds$price)` diamonds.  The population mean of the price of these diamonds (labeled in the plot with the red dot) is $`r format(mean(diamonds$price),nsmall=2)`.  We look for evidence that the sampled-from population consists of diamonds that are more expensive, on average, than the reference population.  In other words, we look for evidence that the population mean of the _new population_ (which we only know of from the sample) is larger than $`r format(mean(diamonds$price),nsmall=2)`, the population mean of the reference population.  The approach we take uses the data we have.

We can identify the following hypotheses:

The _null hypothesis_: the statement that there is no effect:

$$H_0 : \mbox{The sample was drawn from the reference population}$$
Restated, the null hypothesis posits that the new population equals the reference population---and we know exactly what the reference population is.  Thus the null hypothesis is a specific statement.  There is only one way for it to be correct.  Making such a specific statement allows us to quantify the evidence that it is false.

The _alternative hypothesis_: the statement that the effect we want is present:

$$H_a: \mbox{The sample was drawn from a population with a population mean greater than \$`r format(mean(diamonds$price),nsmall=2)`}$$
<span style="color:red"> Question: Is the alternative hypothesis a specific statement?  We said there was only one way for the null hypothesis to be correct, is there more than one way that the alternative hypothesis could be correct? </span>  

Here is an example sample of prices of 4 diamonds:

```{r diasample, echo=F}
sample.size <- 4
set.seed(1)
S4.1 <- sample(diamonds$price, sample.size)
S4.1
```

Is there evidence for our alternative hypothesis?  We are going to use the sample mean to make this decision.  In this case, the sample mean is the sum of these four prices, the only prices we know from new population, divided by 4.  This sample mean is what we call our **test statistic**. <span style="color:red"> Question: If our null hypothesis is correct, what are the likely values of our test statistic?  If our null hypothesis is false, and instead our alternative hypothesis is correct, in what way(s) will the likely values of the test statistic shift? To answer these questions, refer to the plot plot, below.  Why is this plot relevant? What part is relevant to the sample size of 4?  What values for the test statistic would suggest that the null hypothesis is false and instead that the alternative hypothesis is correct?  How does this change with different sample sizes? </span>  

```{r samplesizeplot, echo=F}
diamonds.price.sample.of.n <- function(sample.size) {
  return(sample(diamonds$price, sample.size))
}

m.statistics.of.n <- function(statistic,number.of.samples,sample.size) {
  stats <- c()
  for (k in 1:number.of.samples) {
    stats <- c(stats, statistic(diamonds.price.sample.of.n(sample.size)))
  }
  return(stats)
}

set.seed(2)
m <- 1000
stats <- cbind()
labels <- cbind()
levels <- c(1,4,16,64,256,1024)
for (i in c(1,4,16,64,256,1024)) {
  new.stats <- cbind(m.statistics.of.n(statistic=mean, number.of.samples=m, sample.size=i))
  new.labels <- cbind(rep(as.character(i), m))
  stats <- rbind(stats, new.stats)
  labels <- rbind(labels, new.labels)
}
df <- data.frame(stats,labels)
df$labels <- factor(df$labels,levels=as.character(levels))
ggplot(df, aes(x=labels, y=stats)) + geom_violin() + geom_boxplot(width=0.1) + xlab("Sample Size") + ylab("Sample Mean of Price") + stat_summary(fun.y=mean, geom="point", size=2, color="red") + geom_hline(yintercept=mean(diamonds$price))
```


If you answered the above questions, you may understand why we interpret large sample means as evidence against our null hypothesis and for our alternative hypothesis.  At some point, when the sample mean is large enough, we deem the evidence for the alternative hypothesis **significant**.  But how large does the sample mean need to be to get this conclusion?  The threshold, where we say the sample mean is large enough, is called the **critical value of the test statistic**.

To understand how to set this critical value, let's tackle a simpler problem: our sample will consist of a single diamond: just the first one selected in the sample above, with price: $`r S4.1[1]`.  Based on just this diamond, should we accept our alternative hypothesis?  It can be said that we do have some evidence: $`r S4.1[1]` > `r format(mean(diamonds$price),nsmall=2)`.$  But do we have enough evidence?  And how much evidence is enough?  It depends on our critical value.  If `r S4.1[1]` is bigger than our critical value, then we will say we have enough evidence to conclude that diamond was sampled from a population with a greater mean.  If not, we won't be able to say that. <span style="color:red"> Question: If we had set our critical value for the test statistic to $5600, what would we deem true about our null hypothesis and our alternative hypothesis?  How about if we had set our critical value for the test statistic to $6600? </span> Turns out, there is a principled way of setting this critical value.  How do we do it?

We don't know the distribution of the diamonds from the population from which the sample was drawn, however we **do** know the distribution of diamonds under the null hypothesis.   Therefore, we can answer the following question: **If we make the null hypothesis correct, by drawing a sample from the null population (call it a null-sample), and we use our observed test statistic, $`r S4.1[1]`, as the _critical value_, for deeming the alternative hypothesis correct, what is the probability that we will make such an error?**  (Note: we know the alternative hypothesis is incorrect for null-samples.)  This probability is the **_p-value_** for our original sample.  Said a slightly different way, what is the probability of erroneously concluding that a null-sample comes from a more expensive population, assuming the original sample determines our threshold for making this decision?  This probability is the **_p-value_** for our original sample.  Because **the _p-value_ is a probability**, it is always a number between 0 and 1.  The closer the _p-value_ is to 0, the more surprised we would be to see our data (the original sample) if the null hypothesis were correct.  Low _p-values_ are interpreted as strong evidence against the null hypothesis and for the alternative hypothesis.

```{r meandiawithsample, echo = F}
ggplot(diamonds, aes(x=0, y=price)) + geom_violin() + geom_boxplot(width=0.1) + xlab("") + theme(axis.text.x=element_blank()) + stat_summary(fun.y=mean, geom="point", size=2, color="red") + scale_y_continuous(labels = dollar, breaks = seq(0, 20000, 1000)) + geom_hline(yintercept=S4.1[1])
```

The black horizontal line is drawn at the price of the single sample diamond: $`r S4.1[1]`.  What if, as suggested above, we used this line as the critical value for assessing the truth of the alternative hypothesis?  We have no way of knowing what would happen if the alternative hypothesis were correct.  There are many populations that satisfy the alternative hypothesis.  _But we do know what would happen if the null hypothesis is correct---there is only one population that satisfies the null hypothesis---the one depicted in the violin/box plot, above._   If the black line represented our critical value, and if the price of a null-sampled diamond fell above the black line, we would incorrectly reject the null hypothesis.   <span style="color:red"> Question: Compared to the single diamond price seen in our original sample, $`r S4.1[1]`, in what way could the originally sampled price of the diamond have been different such that it would have led to a lower p-value?  Think carefully about the definition given above and also refer to the immediately previous figure. </span>   Test statistics that lead to lower p-values are said to be **more extreme** (that is, further into the region where we deem the alternative hypothesis correct).  The p-value is also described as the probability of observing a null-sample with a more extreme p-value than seen with the original sample.

The number of diamonds in the reference population whose price fall above this line is `r sum(diamonds$price>S4.1[1])` which is `r format(100*sum(diamonds$price>S4.1[1])/length(diamonds$price),digits=4)`% of the diamonds in the data set.  Since each diamond has an equal chance of being selected as our new samples, the probability of an error, in this situation, is:

$$\mbox{p-value} = \frac{`r sum(diamonds$price>S4.1[1])`}{`r length(diamonds$price)`} = `r format(sum(diamonds$price>S4.1[1])/length(diamonds$price),digits=4)`$$

Erroneously accepting the alternative hypothesis is called a false positive, because we erroneously find the _positive_ effect: the null-sample comes from a more expensive population, but the correct answer is negative: we drew our samples from the null distribution.  The following are examples of positive effects: the new store sells more expensive diamonds than the old store, the new drug works better than placebo, my new teaching methods work better than the old ones, etc.  If we come to these conclusions in error, we have made a false-positive error (type I error).  The other type of error is a false negative error (type II error).  Negating the positive statements yields negative statements (e.g. the new drug works the same as placebo), and if such a conclusion is in error (we conclude that the new drug works the same as placebo, when in fact it works better), our conclusion is a false negative (type II) error.  Tests of significance are designed to control for type I errors (false positives), though we can study false negatives as well (mostly saved for future lessons, but see below).  <span style="color:red"> Question: Why is it easier to control for type I errors than to control for type II errors?  Hint: remember that the null hypothesis is a specific statement about a single distribution or population---there is only one way the null hypothesis can be correct---whereas the alternative hypothesis isn't---there are many ways the alternative hypothesis can be correct. </span>

The _p-value_ is the probability of making a false-positive error---incorrectly accepting the alternative hypothesis---_if the null hypothesis is correct and if we use the test statistic seen in our original sample as the critical value for accepting the alternative hypothesis_.  However the critical value should never be chosen based on the data.  It should be chosen, before collecting data, based on _what probability of a type I error you find acceptable_.  

So what's the answer: do we have evidence to support the alternative hypothesis?  Actually, it depends on you.  If you are O.K. with being wrong `r format(100*sum(diamonds$price>S4.1[1])/length(diamonds$price),digits=4)`% of the time when you make the null hypothesis correct, then yes, you do have enough evidence.

**The _level of significance of a test_, _alpha-level of a test_ or _$\alpha$-level of a test_, is probability of making a false-positive error, assuming the null hypothesis is correct.  The alpha-level of a test is typically decided on, in advance of an experiment, and sets the actual critical value for accepting the alternative hypothesis that is used, regardless of the sample.**

The alpha-level of the test, the acceptable probability of a false-positive error, is usually much lower than `r format(sum(diamonds$price>S4.1[1])/length(diamonds$price),digits=4)`.  In fact, the traditional level, accepted by most statisticians, is 0.05, though other levels are sometimes used.  In other words, many statisticians consider it acceptable to have a 0.05 probability of making a false-positive error, under the condition where the null hypothesis is correct.

So if we set the alpha-level to its traditional level, 0.05, what is the critical value for the test statistic. That's easy: its the value such that 5% of the diamonds have a larger price, and thus 95% of the diamonds have a lower price.  That value has a name: the 95th percentile of the price of diamonds.

```{r percentile95, echo=F}
critical.value <- quantile(diamonds$price, probs=0.95)
names(critical.value) <- NULL
```

The 95th percentile of diamond price in our data set is `r format(critical.value)`.  In other words, if a single diamond, sampled at random from a different data set, such as from a different store, has a price that is greater than `r format(critical.value)`, then statisticians would, based on tradition, consider the evidence sufficient to conclude that the diamonds from the new store are more expensive than the diamonds from the old store.  However, if we trick the statisticians, and give them a diamond from the old store, so that a positive conclusion (diamonds are more expensive from the population sampled) would be wrong, the statisticians would be wrong 5% of the time.  They would know that fact, but consider this rate of false-positive error acceptable.

```{r meandiawithsampleandcriticalvalue, echo = F}
ggplot(diamonds, aes(x=0, y=price)) + geom_violin() + geom_boxplot(width=0.1) + xlab("") + theme(axis.text.x=element_blank()) + stat_summary(fun.y=mean, geom="point", size=2, color="red") + scale_y_continuous(labels = dollar, breaks = seq(0, 20000, 1000)) + geom_hline(yintercept=S4.1[1])+geom_hline(yintercept=critical.value, color='red')
```

The black line (at $`r S4.1[1]`) indicates the price of the diamond sampled.  The _p-value_ is the fraction of diamonds in the reference (null) distribution that fall above the black line.  But location of the black line depends on which diamond is chosen for the sample, and this diamond is chosen from a completely unknown distribution.  In fact, we have no idea where this line is likely to fall.  But we interpret its location based on the null distribution.  If the null distribution were correct, how surprised would we be to find the black line where we find it: our surprise is quantified by the p-value.

The red line (at $`r format(critical.value)`) indicates the critical value of the test statistic, determined by the alpha-level of the test (chosen in accordance with tradition to be 0.05), so that 5% of the diamonds in the null distribution lie above the line.  Since the black line is below the red line, (the p-value is greater than 0.05), we do not deem the evidence sufficient to conclude our alternative hypothesis that the diamonds are more expensive, on average, at the new store.  This is said to be a non-significant result, and that the data are not significant.

On the other hand if the black line were higher than the red line, the p-value would be less than 0.05, and we would accept the alternative hypothesis.  In this case we would find a significant result, and the data would be significant.
<span style="color:red"> Question: If we set the alpha-level higher, say to 0.1, would it be easier or harder to make a type I error?  Would it be easier or harder to find a significant result? </span>

If the null hypothesis is correct, then we erroneously get a type I error 5% of the time, with a 0.05 level of significance.  If the alternative hypothesis is correct (the mean of the sampled-from distribution is larger than the mean of the null distribution) then we would usually expect to find a significant effect _more than 5% of the time_.  If we knew the distribution of the sampled-from distribution, we could compute this number exactly: it is called the **power** of the test.  

**The power of a test is the probability that a sample will be significant, assuming the alternative hypothesis is correct.  To compute the power, you must assume you know the distribution of the sampled-from distribution that satisfies the alternative hypothesis.**

Actually, to be precise, the alternative hypothesis is framed in terms of the _mean_ of the sample-from distribution, whereas the power of the test depends on the percentile of the critical value of the test statistic under the sampled-from distribution, which may or may not be bigger than 5%, but usually is substantially bigger than 5%.  A power of 80% is considered good, although many times statisticians settle for much lower power.

The probability of a type I error is always the alpha-level of the test, often 0.05.  The probability of a type II error is $1 - \mbox{power}$.  If the power is 80%, (or 0.8) the probability of a type II error is 0.2.  The power depends on many things, notably the sample-from distribution.  But the best way to control power is to tweak your sample size.
<span style="color:red"> Question: If we set the alpha-level higher, say to 0.1, would the power go up or down?  Would it be easier or harder to find a significant result? </span>

Next up: we will start thinking about what happens to power as we increase the sample size, and why.  I saw this written on the wall of a booth in the library a few years ago: "I heard you upped your sample size.  More power to you!"  We want to understand this statement, and why its true.